### 停机维护需要先关闭数据平衡
    sudo ceph osd set noout
    sudo ceph osd set norebalance
    sudo ceph osd set norecover
    完毕后再开启
    sudo ceph osd unset noout
    sudo ceph osd unset norebalance
    sudo ceph osd unset norecover

### 重启osd 进程后报没有权限
    2018-01-26 13:50:53.120884 7f541a52de00 -1 bluestore(/var/lib/ceph/osd/ceph-0/block) _read_bdev_label failed to open /var/lib/ceph/osd/ceph-0/block: (13) Permission denied
    先查看 /var/lib/ceph/osd/ceph-0/block 指向的位置
       readlink -f /var/lib/ceph/osd/ceph-0/block
       ls -la /dev/dm-1
    修改改ceph.ceph
       chown ceph.ceph /dev/dm-1

### 查询osd 配置参数
    ceph tell osd.* config show
    
### 重启节点后输入ceph 命令卡死
    查看日志有 8月 27 23:20:10 ceph2 ceph-osd[111419]: 2022-08-27T23:20:10.965+0800 7f615bce2bc0 -1 osd.8 3272 unable to obtain rotating service keys; retrying 错误 
    大概率是时间不同步误差较大需要强制同步一下时间
    chronyc -a makestep
    重启相关服务
    
### vm MapVolume 失败报错 rbd: sysfs write failed
    1,vm所在的机器内核不对，升级内核至5.4版本，重启服务器
    2,有可能是客户端被加入黑名单了 查看黑名单 ceph osd blacklist ls  清除黑名单 ceph osd blacklist clear

### 挂载的共享目录出现没有权限操作，目录所属用户，用户组都变成 ？号
    这是因为触发的ceph 底层黑名单机制 详细请看 https://drunkard.github.io/cephfs/eviction/
    1, 清除黑名单 ceph osd blacklist clear(ceph osd blacklist rm 0.0.0.0:0/sdererer)
    2, 重新 mount 
    3, 重启 mds 服务   systemctl restart ceph-mds@hostname
    
### rbd map 失败 MapVolume.SetUpDevice failed for volume "pvc-zhouhe" : rpc error: code = Internal desc = rbd: map failed with error an error (exit status 108) occurred while running rbd args: [--id kube -m 10.253.69.173:6789,10.253.69.174:6789,10.253.69.175:6789 --keyfile=***stripped*** map kube/pvc-zhouhe --device-type krbd], rbd error output: rbd: sysfs write failed rbd: map failed: (108) Cannot send after transport endpoint shutdown
    1，清除黑名单 ceph osd blacklist clear(ceph osd blacklist rm 0.0.0.0:0/sdererer)
    2，查看是否存在独占锁 rbd lock ls kube/pvc-zhouhe
    3，删除独占锁 rbd lock rm kube/pvc-zhouhe "auto 18446462598732841922" client.6636397


### 部署ceph时ceph-volume --cluster ceph lvm batch  --bluestore  --yes /dev/bcache0 /dev/bcache1 卡主不动
    1，多半是因为 osd 盘容量不同导致的
    2, 如果因为卸载了重新装二卡死，那就把机器都重启一遍

### rbd map 命令卡死没有反应
    1， systemctl restart systemd-udevd.service（不知道原因，但是重启服务肯定好使[但是现在看也不好使了]）
###  MapVolume.SetUpDevice failed for volume "pvc-97af1fa7-cbd0-3ead-d185-5f5f265f0369" : rpc error: code = Aborted desc = an operation with the given Volume ID pvc-97af1fa7-cbd0-3ead-d185-5f5f265f0369 already exists
    1, 先找调度到那个节点上了
    2，登陆节点 查看是否有 rbd map 命令卡死了（systemctl restart systemd-udevd.service 重启次服务）
    3，删除 /var/lib/kubelet/plugins/kubernetes.io/csi/ 文件夹下 pvc-97af1fa7-cbd0-3ead-d185-5f5f265f0369 目录
    4，删除 此节点上的 csi-rbdplugin pod(观察一会看看，如果还不行接着往下看)
    5, 先看虚拟机调度到哪台节点上，登陆节点 查看内核日志 journalctl -xef 看有么有 类似于 rbd: rbd0: no lock owners detected 这样的日志
    6, 查看是否存在独占锁 rbd lock ls kube/pvc-zhouhe
    7, 删除独占锁 rbd lock rm kube/pvc-zhouhe "auto 18446462598732841922" client.6636397
    8, 观察一会看看是否正常，如果不正常，终极大法 重启对应的节点服务器

